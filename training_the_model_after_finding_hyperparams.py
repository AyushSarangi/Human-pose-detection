# -*- coding: utf-8 -*-
"""Training the model after finding hyperparams.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158YstYx2k-K2MRa2m7CGgVQIqrPdEzgG
"""

!pip install -U "ray[data,train,tune,serve]"

from google.colab import drive
drive.mount('/content/drive')

import torch
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms, utils
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from PIL import Image
import os
import numpy as np
import json
import matplotlib.pyplot as plt
from torch.utils.data.dataloader import default_collate

from functools import partial
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import random_split
import torchvision
import torchvision.transforms as transforms
from ray import tune
from ray.train import Checkpoint
from ray.air import  session
from ray.tune.schedulers import ASHAScheduler

# Define the dataset class
class HumanPoseDataset(Dataset):
    def __init__(self, annotations, img_dir, transform=None):
        self.annotations = annotations
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        img_key = list(self.annotations.keys())[idx]
        annotation_list = self.annotations[img_key]
        # Skip the image if there are no annotations
        if not annotation_list:
            return None
        # Use the first annotation for simplicity
        annotation = annotation_list[0]
        if not annotation['landmarks']:  # Check if landmarks are not empty
            return None
        img_name = os.path.join(self.img_dir, annotation['file'])
        image = Image.open(img_name).convert('RGB')
        original_image_size = image.size
        keypoints = annotation['landmarks']
        keypoints_array = np.array([[k['x'], k['y'], k['z'], k['visibility']] for k in keypoints])

        if self.transform:
            image = self.transform(image)

        sample = {'image': image, 'keypoints': keypoints_array, 'original_image_size': original_image_size}
        print(sample)
        return sample

# Custom collate function to filter out None values
def custom_collate(batch):
    batch = [b for b in batch if b is not None]
    return default_collate(batch)

def load_data(img_dir, annotations_path ):

  with open(annotations_path) as f:
    annotations_data = json.load(f)

  # Define the transformations with resizing and augmentation
  transform = transforms.Compose([
      transforms.Resize((32, 32)),  # Resize the images to 256x256
      transforms.ToTensor(),
      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
      transforms.RandomHorizontalFlip(),  # Example augmentation
      # Add more augmentations if needed
  ])

  test_transform=transforms.Compose([
      transforms.ToTensor(),
      transforms.Resize((1920,1080)),
  ])

  # Create the dataset
  human_pose_dataset = HumanPoseDataset(annotations_data, img_dir, transform=transform)
  testing_pose_dataset = HumanPoseDataset(annotations_data, img_dir, transform=test_transform)

  train_size = int(0.8* len(human_pose_dataset))
  validation_size = int(0.1 * len(human_pose_dataset))
  test_size = len(human_pose_dataset) - train_size - validation_size
  train_dataset, remaining_dataset = random_split(human_pose_dataset, [train_size, validation_size + test_size])
  validation_dataset, test_dataset = random_split(remaining_dataset, [validation_size, test_size])

  test_pose_dataset , remaining_data = random_split(testing_pose_dataset,[6,194])

  batch_size =4

  # Create data loaders for each set with the custom collate function
  train_loader = DataLoader(train_dataset, batch_size= batch_size, shuffle=True, collate_fn=custom_collate)
  validation_loader = DataLoader(validation_dataset, batch_size= batch_size, shuffle=True, collate_fn=custom_collate)
  test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False, collate_fn=custom_collate)

  test_image_loader = DataLoader(test_pose_dataset, batch_size= batch_size, shuffle=False, collate_fn=custom_collate)

  return train_loader,validation_loader,test_loader,test_image_loader

# using Kaiming Initialisation

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(256)

        # Assuming the input image size is 32x32, after four pooling layers the image size will be 2x2
        self.fc1 = nn.Linear(256 * 2 * 2, 1024)
        # self.bn_fc1 = nn.BatchNorm1d(1024)
        self.fc2 = nn.Linear(1024, 640)

        self.fc3 = nn.Linear(640, 33 * 4)  # Assuming 33 keypoints

        self.initialise_weight()

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.bn1(x)
        x = self.pool(F.relu(self.conv2(x)))
        x = self.bn2(x)
        x = self.pool(F.relu(self.conv3(x)))
        x = self.bn3(x)
        x = self.pool(F.relu(self.conv4(x)))
        #x = self.bn4(x)

        x = torch.flatten(x, 1)  # Flatten the tensor for the fully connected layer

        x = F.relu(self.fc1(x))
        #x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        #x = self.dropout2(x)
        x = self.fc3(x)
        return x

    def initialise_weight(self):
      for m in self.modules():
        if isinstance(m, nn.Conv2d):
          nn.init.kaiming_uniform(m.weight)

          if m.bias is not None:
            nn.init.normal(m.bias, mean =0 )

        elif isinstance(m, nn.Linear):
          nn.init.kaiming_uniform(m.weight)


# Initialize the model
model = SimpleCNN()
print("Model initialized.")
print(model)  # Print the model architecture

img_dir = '/content/drive/MyDrive/CNN_updated_Dataset'
annotations_path  = '/content/drive/MyDrive/annotations_CNN (3).json'

trainset, valset, _, _= load_data(img_dir, annotations_path)

sample_batch = next(iter(trainset))
images = sample_batch['image'].float()
keypoints = sample_batch['keypoints'].view(-1, 132).float()

validation_sample_batch = next(iter(valset))
validation_images = validation_sample_batch['image'].float()
validation_keypoints = validation_sample_batch['keypoints'].view(-1, 132).float()

optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = wd)
criterion = torch.nn.MSELoss()

train_loss = []
val_loss = []

for epoch in range(150):  # loop over the dataset multiple times

    optimizer.zero_grad()
    outputs = model(images)
    train_current_loss = criterion(outputs, keypoints)

    train_current_loss.backward()
    optimizer.step()


    model.eval()  # Switch to evaluation mode for validation
    with torch.no_grad():
            # Calculate validation loss
        val_outputs = model(validation_images)
        val_current_loss = criterion(val_outputs, validation_keypoints)

    print(f"Epoch [{epoch + 1}/100], Loss: {train_current_loss.item():.4f}, Val Loss: {val_current_loss.item():.4f}")
    train_loss.append(train_current_loss.item())
    val_loss.append(val_current_loss.item())

plotting_val_loss = val_loss
plotting_train_loss = train_loss

plt.figure(figsize=(8, 4))

plt.plot( plotting_train_loss, marker='o', linestyle='-', color='b',label='train loss')
plt.plot( plotting_val_loss, marker='o', linestyle= '-', color='r', label='val loss')

plt.title('Loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid(True)
plt.legend()

# Show the legend in a small box
plt.legend(loc='upper right')

plt.show()

val_loss = np.average(np.array(val_loss))
train_loss = np.average(np.array(train_loss))

print(val_loss)
print(train_loss)

# Ensure that the tensors are also floats

_, _, test_loader, _ = load_data(img_dir, annotations_path)
sample_batch = next(iter(test_loader))

test_images = sample_batch['image'].float()  # Convert images to float
test_keypoints = sample_batch['keypoints'].view(-1, 132).float()  # Convert keypoints to float and reshape

model.eval()

optimizer.zero_grad()
outputs = model(test_images)

print("Testing Done")

test_images.shape
test_actual_plot = test_keypoints.reshape(len(test_images),33,4)[0]
test_predict_plot = outputs.reshape(len(test_images),33,4)[0]
test_predict_plot.shape

import cv2
import matplotlib.pyplot as plt
import numpy as np

def plot_human_pose(keypoints):
    # Create a figure and axis
    fig, ax = plt.subplots()

    # Plot keypoints
    for i in range(len(keypoints)):
        x, y, _, _ = keypoints[i]
        ax.scatter(x, -y, color='blue')  # Invert y-axis

    # Connect body parts
    connect_lines = [(0, 2), (2, 7),   # Left eye
                     (0, 5), (5, 8),   # Right eye
                     (9,10),  # Left side
                     (11, 12), (12, 24), (11, 23),  # Right side
                     (24,23), (24,26), (23,25),  # Connect ears and wrists
                     (26, 28), (25, 27),
                     (28, 30), (28, 32), (30,32),# Connect left and right pinky fingers
                     (27, 29), (27, 31), (31,29),  # Connect left and right index fingers
                     (12, 14), (11, 13),  # Connect left and right thumbs
                     (14, 16), (13, 15),  # Connect left and right hips
                     (16, 18), (18, 20), (16,20), (16,22),  # Connect left and right knees
                     (15, 17), (15, 19),  # Connect left and right ankles
                     (17, 19), (15, 21)]  # Connect left and right heels

    for line in connect_lines:
        start, end = line
        x_vals = [keypoints[start][0], keypoints[end][0]]
        y_vals = [-keypoints[start][1], -keypoints[end][1]]  # Invert y-axis
        ax.plot(x_vals, y_vals, linewidth=2, color='red')

    ax.set_aspect('equal', adjustable='datalim')
    plt.title('Actual Pose')
    plt.axis('off')
    plt.show()

# Example usage:
keypoints = test_actual_plot  # Replace with your 33 key points
plot_human_pose(keypoints)

from io import BytesIO

def plot_human_pose(keypoints):
    # Create a figure and axis
    fig, ax = plt.subplots()

    # Plot keypoints
    for i in range(len(keypoints)):
        x, y, _, _ = keypoints[i]
        ax.scatter(x, -y, color='blue')  # Invert y-axis

    # Connect body parts
    connect_lines = [(0, 2), (2, 7),   # Left eye
                     (0, 5), (5, 8),   # Right eye
                     (9,10),  # Left side
                     (11, 12), (12, 24), (11, 23),  # Right side
                     (24,23), (24,26), (23,25),  # Connect ears and wrists
                     (26, 28), (25, 27),
                     (28, 30), (28, 32), (30,32),# Connect left and right pinky fingers
                     (27, 29), (27, 31), (31,29),  # Connect left and right index fingers
                     (12, 14), (11, 13),  # Connect left and right thumbs
                     (14, 16), (13, 15),  # Connect left and right hips
                     (16, 18), (18, 20), (16,20), (16,22),  # Connect left and right knees
                     (15, 17), (15, 19),  # Connect left and right ankles
                     (17, 19), (15, 21)]  # Connect left and right heels

    for line in connect_lines:
        start, end = line
        x_vals = [keypoints[start][0], keypoints[end][0]]
        y_vals = [-keypoints[start][1], -keypoints[end][1]]  # Invert y-axis
        ax.plot(x_vals, y_vals, linewidth=2, color='green')

    ax.set_aspect('equal', adjustable='datalim')
    plt.title('Predicted Pose')
    plt.axis('off')

    buffer = BytesIO()
    plt.savefig(buffer, format="png")
    buffer.seek(0)  # Reset the buffer position to the beginning

    # Close the plot to release resources
    plt.close()

    return Image.open(buffer)

# Example usage:
keypoints = test_predict_plot.detach().numpy() # Replace with your 33 key points
plot_human_pose(keypoints)









